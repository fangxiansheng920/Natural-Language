# Natural-Language
对于自然语言处理总结
### 自然语言处理编程文档（A部分）

#### 一、功能概述
自然语言处理功能，包含以下模块：
1. **分词与词性标注**：基于jieba库实现文本分词及词性标注，支持全模式、精确模式、搜索引擎模式。
2. **关键词提取**：通过TF-IDF算法抽取文本核心关键词。
3. **词频统计与可视化**：生成词云、饼图、柱状图、折线图等多维度数据可视化。
4. **自定义词典扩展**：支持用户自定义词典提升专业领域分词精度。
5. **实体统计**：按人名（nr）、地名（ns）等词性分类存储。

#### 二、设计思想
1. **模块化设计**：每个功能封装为独立函数，通过主程序调用。
2. **可扩展性**：通过`jieba.load_userdict()`支持用户自定义词典，适应不同领域需求。
3. **数据流控制**：采用“输入-处理-输出”流程，输入支持文件/字符串，输出包含文本文件及可视化图表。

#### 三、核心库及函数
1.jieba库:中文分词与词性标注。函数：cut()    
2.wordcloud库:词云生成。函数：WordCloud()   
3.matplotlib库:数据可视化。函数：pyplot.pie(),pyplot.bar(),pyplot.plot,pyplot.scatter
4.numpy库:图像处理与数组操作。函数：np.array()   

#### 四、测试数据与结果
1. **输入示例**：  
   ```python
   s1 = '我想和女朋友一起去北京故宫博物院参观和闲逛。'
   ```
2. **输出结果**：  
   - 精确模式分词：`我,想,和,女朋友,一起,去,北京故宫博物院,参观,和,闲逛`  
   - 全模式分词：`我,想,和,女,朋友,一起,去,北京,故宫,博物院,参观,和,闲,逛` 
   - 词性标注：`我/r, 想/v, 和/c, 女朋友/n, 去/v, 北京故宫博物院/ns`  
   - 关键词提取：`博物院, 闲逛, 参观`  

---

### 功能完善方案（B部分）

#### 模块化函数设计
```python
def tokenize(text, mode='exact'):
    """分词功能：mode可选'exact'（精确）、'all'（全模式）"""
    if mode == 'exact':
        return jieba.cut(text, cut_all=False)
    elif mode == 'all':
        return jieba.cut(text, cut_all=True)

def count_word_freq(words, topk=10):
    """词频统计：返回前topk高频词"""
    tf_dict = {}
    for word in words:
        tf_dict[word] = tf_dict.get(word, 0) + 1
    return sorted(tf_dict.items(), key=lambda x: x[1], reverse=True)[:topk]

def save_entities(text, entity_type='nr', filename='result.txt'):
    """实体分类保存：按词性（nr/ns等）筛选"""
    words = pseg.cut(text)
    entities = [w.word for w in words if w.flag == entity_type]
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(entities))
```

#### 主程序调用示例
```python
if __name__ == "__main__":
    text = open('input.txt', 'r', encoding='utf-8').read()
    
    # 分词与词频统计
    words = list(tokenize(text, mode='exact'))
    print("高频词:", count_word_freq(words))
    
    # 生成词云
    generate_wordcloud(' '.join(words))
    
    # 保存地名实体
    save_entities(text, entity_type='ns', filename='places.txt')
```

---

### 界面设计方案（C部分）

#### 方案选择：Tkinter
采用Python内置Tkinter库实现桌面GUI，优点包括：
1. **零依赖**：无需额外安装框架，适合课程作业部署[48](@ref)。
2. **功能完备**：支持文件选择、参数配置、结果展示控件。

#### 核心组件设计
```python
import tkinter as tk
from tkinter import filedialog

class NLPApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("NLP工具箱")
        self.geometry("800x600")
        
        # 文件选择按钮
        self.btn_file = tk.Button(self, text="选择文件", command=self.load_file)
        self.btn_file.pack(pady=10)
        
        # 功能选择下拉菜单
        self.mode_var = tk.StringVar()
        self.mode_menu = tk.OptionMenu(self, self.mode_var, 
                                      '分词', '词云', '实体统计')
        self.mode_menu.pack(pady=5)
        
        # 结果显示文本框
        self.text_result = tk.Text(self, wrap=tk.WORD)
        self.text_result.pack(fill=tk.BOTH, expand=True)
    
    def load_file(self):
        filepath = filedialog.askopenfilename()
        text = open(filepath, 'r', encoding='utf-8').read()
        self.process_text(text)
    
    def process_text(self, text):
        mode = self.mode_var.get()
        if mode == '分词':
            result = ','.join(jieba.cut(text))
            self.text_result.insert(tk.END, result)

if __name__ == "__main__":
    app = NLPApp()
    app.mainloop()
```

---

### 文档与代码提交说明
1. **代码结构**  
   ```
   /project
   ├── nlp_core.py    # 核心功能函数
   ├── gui.py         # 界面实现
   ├── requirements.txt
   └── docs/          # 实验报告与截图
   ```

2. **测试数据**  
   - 输入文件：`input.txt`（UTF-8编码中文文本）
   - 输出文件：`words.csv`（词频统计）、`places.txt`（地名实体）

3. **评分要点**  
   - **代码质量**：模块化、注释完整性（参考[70](@ref)代码结构规范）  
   - **功能实现**：分词准确性、可视化美观度  
   - **文档完整性**：需求分析、设计思路、结果截图  

完整代码与文档示例可参考GitHub仓库：https://github.com/your_repo/nlp_project
