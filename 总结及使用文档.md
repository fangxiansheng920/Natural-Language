# Natural-Language
对于自然语言处理总结
### 自然语言处理编程文档（A部分）

#### 一、功能概述
自然语言处理功能，包含以下模块：
1. **分词与词性标注**：基于jieba库实现文本分词及词性标注，支持全模式、精确模式、搜索引擎模式。 
2. **词频统计与可视化**：生成词云、饼图、柱状图、折线图等多维度数据可视化。
3. **自定义词典扩展**：支持用户自定义词典提升专业领域分词精度。
4. **实体统计**：按人名（nr）、地名（ns）等词性分类存储。

#### 二、设计思想
1. **模块化设计**：每个功能封装为独立函数，通过主程序调用。
2. **可扩展性**：通过`jieba.load_userdict()`支持用户自定义词典，适应不同领域需求。
3. **数据流控制**：采用“输入-处理-输出”流程，输入支持文件/字符串，输出包含文本文件及可视化图表。

#### 三、核心库及函数
1. **jieba库**:中文分词与词性标注。函数：cut()    
2. **wordcloud库**:词云生成。函数：WordCloud()   
3. **matplotlib库**:数据可视化。函数：pyplot.pie(),pyplot.bar(),pyplot.plot,pyplot.scatter
4. **numpy库**:图像处理与数组操作。函数：np.array()   

#### 四、测试数据与结果
1. **输入示例**：  
   ```python
   s1 = '我想和女朋友一起去北京故宫博物院参观和闲逛。'
   ```
2. **输出结果**：  
   - 精确模式分词：`我,想,和,女朋友,一起,去,北京故宫博物院,参观,和,闲逛`  
   - 全模式分词：`我,想,和,女,朋友,一起,去,北京,故宫,博物院,参观,和,闲,逛` 
   - 词性标注：`我/r, 想/v, 和/c, 女朋友/n, 去/v, 北京故宫博物院/ns`   

---

### 功能完善方案（B部分）

#### 模块化函数设计
```python
import jieba
import jieba.posseg as pseg
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import networkx as nx
import os

# 自定义词典管理
def manage_dict(custom_dict_path):
    jieba.load_userdict(custom_dict_path)

# 分词功能
def tokenize(text):
    return list(jieba.cut(text))

# 词频统计
def word_freq(words, topN=20):
    return Counter(words).most_common(topN)

# 词性分类保存
def save_pos(text, output_path):
    words = pseg.cut(text)
    with open(output_path, 'w', encoding='utf-8') as f:
        for word, flag in words:
            f.write(f"{word}\t{flag}\n")

# 可视化功能组
def generate_pie(data, title, output_path):
    labels, values = zip(*data)
    plt.pie(values, labels=labels, autopct='%1.1f%%')
    plt.title(title)
    plt.savefig(output_path)
    plt.close()

def generate_wordcloud(text, output_path):
    wc = WordCloud(font_path='simhei.ttf').generate(text)
    wc.to_file(output_path)

# 实体统计
def count_entities(text, entity_type):
    words = pseg.cut(text)
    return [word for word, flag in words if flag == entity_type]

# 主程序示例
if __name__ == "__main__":
    # 初始化配置
    jieba.initialize()
    
    # 加载自定义词典
    manage_dict('custom_dict.txt')
    
    # 读取文本
    with open('input.txt', 'r', encoding='utf-8') as f:
        text = f.read()
    
    # 执行处理流程
    words = tokenize(text)
    freq_data = word_freq(words)
    save_pos(text, 'pos_tags.txt')
    generate_pie(freq_data[:5], '词频分布', 'word_freq_pie.png')
```

---

### 界面设计方案（C部分）

#### 方案选择：Tkinter
采用Python内置Tkinter库实现桌面GUI，优点包括：
1. **零依赖**：无需额外安装框架，适合课程作业部署。
2. **功能完备**：支持文件选择、参数配置、结果展示控件。

#### 核心组件设计
```python
import tkinter as tk
from tkinter import filedialog
from PIL import ImageTk, Image

class NLPApp:
    def __init__(self):
        self.window = tk.Tk()
        self.window.title('NLP处理系统 v1.0')
        
        # 界面组件
        self.create_widgets()
        
    def create_widgets(self):
        # 文件选择
        self.btn_select = tk.Button(text="选择文件", command=self.load_file)
        self.btn_select.pack(pady=10)
        
        # 功能按钮组
        self.btn_tokenize = tk.Button(text="执行分词", command=self.do_tokenize)
        self.btn_tokenize.pack(pady=5)
        
        # 结果显示区域
        self.text_result = tk.Text(height=15)
        self.text_result.pack()
        
    def load_file(self):
        self.filepath = filedialog.askopenfilename()
        
    def do_tokenize(self):
        if hasattr(self, 'filepath'):
            with open(self.filepath, 'r', encoding='utf-8') as f:
                text = f.read()
            words = tokenize(text)
            self.text_result.insert(tk.END, f"分词结果：{', '.join(words[:50])}...")
            
    def run(self):
        self.window.mainloop()

if __name__ == "__main__":
    app = NLPApp()
    app.run()
```

---


